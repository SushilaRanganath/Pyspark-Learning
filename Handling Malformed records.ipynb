{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3cb90dc-49f9-46f9-a190-2ca3c4d8da21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Malformed records are rows that do not match the expected schema or structure, which can cause parsing errors.\n",
    "\n",
    "# 1. Using the mode Option\n",
    "The mode option allows you to specify how to handle malformed records. The available modes are:\n",
    "\n",
    "#PERMISSIVE (default): Puts malformed records in a separate column.\n",
    "#DROPMALFORMED: Drops malformed records.\n",
    "#FAILFAST: Throws an exception when it encounters malformed records.\n",
    "#Example:\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"HandleMalformedRecords\").getOrCreate()\n",
    "\n",
    "# Read CSV file with mode option\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\",\n",
    "                    header=True,\n",
    "                    mode=\"DROPMALFORMED\",\n",
    "                    inferSchema=True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "709bbafb-dd2e-4697-977f-608dd4ca2a0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PERMISSIVE Mode with Corrupt Column\n",
    "\n",
    "#When using the PERMISSIVE mode, you can specify an extra column to store malformed records by using the columnNameOfCorruptRecord option.\n",
    "\n",
    "# # Read CSV file in PERMISSIVE mode with corrupt record column\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\",\n",
    "                    header=True,\n",
    "                    mode=\"PERMISSIVE\",\n",
    "                    columnNameOfCorruptRecord=\"corrupt_record\",\n",
    "                    inferSchema=True)\n",
    "\n",
    "df.show(truncate=False)\n",
    "#In this example, malformed records are stored in the corrupt_record column, allowing you to inspect and handle them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2fb0f0d-42e2-4671-a372-f75453b6642c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Handling delimiters issues.\n",
    "\n",
    "## Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"HandleDelimiter\").getOrCreate()\n",
    "\n",
    "# Define schema (if needed)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"email\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read CSV file with custom delimiter and quote character\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\",\n",
    "                    schema=schema,\n",
    "                    header=True,\n",
    "                    delimiter=\";\",  # Specify the custom delimiter\n",
    "                    quote='\"',     # Specify the quote character\n",
    "                    escape=\"\\\\\",   # Specify the escape character\n",
    "                    inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b13df34-2aa7-4b6e-8c08-87cb2b9c06c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Handling Multiline records issues.\n",
    "#  Using the multiLine Option\n",
    "\n",
    "#The multiLine option allows Spark to handle records that span multiple lines. When enabled, Spark reads the entire file as a single input and then parses it accordingly.\n",
    "\n",
    "Example:# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"HandleMultilineRecords\").getOrCreate()\n",
    "\n",
    "# Read CSV file with multiline option\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\",\n",
    "                    header=True,\n",
    "                    multiLine=True,  # Enable multiline record handling\n",
    "                    inferSchema=True)\n",
    "\n",
    "                    #Combining multiLine with Other Options\n",
    "When dealing with multiline records, itâ€™s often necessary to combine the multiLine option with other options like quote, escape, delimiter, and header to ensure proper parsing.\n",
    "\n",
    "# Handling Large Files Efficiently\n",
    "Reading large CSV files with multiline records can be resource-intensive. To optimize performance, consider tuning Spark configurations and partitioning the data.\n",
    "\n",
    "## Initialize SparkSession with tuned configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HandleMultilineRecords\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\ ---are tuned to optimize the large reading files\n",
    "    .config(\"spark.sql.files.openCostInBytes\", \"4MB\") \\ --- are tuned to optimize large reading files\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read CSV file with multiline option\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\",\n",
    "                    header=True,\n",
    "                    multiLine=True,\n",
    "                    inferSchema=True)\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "\n",
    "#Example CSV File (path/to/csvfile.csv)\n",
    "\n",
    "#id,name,description,email\n",
    "1,John Doe,\"John is a developer\n",
    "who works on various projects.\",john@example.com\n",
    "2,Jane Smith,\"Jane is a data scientist.\n",
    "She specializes in machine learning.\",jane@example.com\n",
    "\n",
    "#In this example:\n",
    "\n",
    "The CSV file contains multiline records in the description field.\n",
    "The multiLine=True option allows Spark to read the entire record correctly, even though it spans multiple lines.\n",
    "Custom delimiter and quote character settings ensure the data is parsed accurately.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Handling Malformed records",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
