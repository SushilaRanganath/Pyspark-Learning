{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0948fb1b-9185-41da-bc6e-a085e5b6ed60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Handling nulls and missing values effectively is crucial when reading CSV files into Spark DataFrames to ensure data integrity and prevent errors during data processing.\n",
    "\n",
    "#1. Identifying Null Values\n",
    "When reading a CSV file, you might encounter different representations of null values, such as NA, NULL, empty strings, or other placeholders. Spark provides options to handle these representations appropriately.\n",
    "\n",
    "#2. Using nullValue Option\n",
    "The nullValue option allows you to specify what should be considered as a null value in the CSV file.\n",
    "example.\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"HandleNulls\").getOrCreate()\n",
    "\n",
    "# Reading CSV with nullValue option\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\",\n",
    "                    header=True,\n",
    "                    nullValue=\"NA\",\n",
    "                    inferSchema=True)\n",
    "\n",
    "df.show()\n",
    "\n",
    "#In this example, any occurrence of NA in the CSV file will be treated as a null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db3286f4-c294-462d-8e8f-f135c071db9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using 'Nanvalue' option\n",
    "#the 'NanValue' option is used to specify what should be considered as 'NaNvalue'(Not a Number)\n",
    "\n",
    "# Reading CSV with nullValue and nanValue options\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\",\n",
    "                    header=True,\n",
    "                    nullValue=\"NA\",\n",
    "                    nanValue=\"NaN\",\n",
    "                    inferSchema=True)\n",
    "\n",
    "df.show()\n",
    "#In this example, both NA and NaN in the CSV file will be treated as null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2755c3d-5672-4b52-acd7-3fd4acf91d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dealing with missing values post read.\n",
    "\n",
    "#Once the DataFrame is loaded, Spark provides various methods to handle missing values, such as 'fillna', 'dropna', and 'replace'.\n",
    "\n",
    "# Fill null values with a specific value\n",
    "df_filled = df.fillna({\"age\": 0, \"name\": \"Unknown\", \"email\": \"no_email@example.com\"})\n",
    "\n",
    "df_filled.show()\n",
    "\n",
    "#In this example, null values in the age column are replaced with 0, in the name column with \"Unknown\", and in the email column with \"no_email@example.com\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1091a46c-6831-4131-a3ae-f3a4a786ab2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows where any value is null\n",
    "df_dropped = df.dropna()\n",
    "\n",
    "df_dropped.show()\n",
    "\n",
    "## Drop rows where all values are null\n",
    "df_dropped_all = df.dropna(how='all')\n",
    "\n",
    "df_dropped_all.show()\n",
    "\n",
    "## Drop rows that have less than 2 non-null values\n",
    "df_dropped_thresh = df.dropna(thresh=2)\n",
    "\n",
    "df_dropped_thresh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11bfd83f-0241-4a19-8812-35e838f2890a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Replacing Specific Values\n",
    "#Sometimes, you might want to replace specific values, such as empty strings or specific placeholders, with nulls.\n",
    "\n",
    "Example:\n",
    "#    from pyspark.sql.functions import col, when\n",
    "\n",
    "# Replace empty strings with null\n",
    "df_replaced = df.withColumn(\"name\", when(col(\"name\") == \"\", None).otherwise(col(\"name\")))\n",
    "\n",
    "df_replaced.show()\n",
    "#In this example, empty strings in the name column are replaced with null."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Handling nulls while read csv file into a Dataframe",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
