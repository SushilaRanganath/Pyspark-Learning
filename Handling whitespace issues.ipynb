{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "573292ea-ee0b-4152-8c91-858d6a704171",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "#Handling whitespace issues is important when reading CSV files into Spark DataFrames, as leading and trailing whitespaces in data can lead to inaccuracies and misinterpretations.\n",
    "\n",
    " #Using ignoreLeadingWhiteSpace and ignoreTrailingWhiteSpace Options\n",
    "#The ignoreLeadingWhiteSpace and ignoreTrailingWhiteSpace options can be used to trim leading and trailing whitespaces from fields.\n",
    "\n",
    "# # Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"HandleWhitespace\").getOrCreate()\n",
    "\n",
    "# Read CSV file with whitespace handling options\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\",\n",
    "                    header=True,\n",
    "                    ignoreLeadingWhiteSpace=True,  # Ignore leading whitespaces\n",
    "                    ignoreTrailingWhiteSpace=True,  # Ignore trailing whitespaces\n",
    "                    inferSchema=True)\n",
    "\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76ac75a8-3f5d-4c83-a02d-577ae97f7b2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using trim Method on DataFrame Columns\n",
    "\n",
    "#You can manually trim whitespaces from specific columns using the trim method from pyspark.sql.functions.\n",
    "\n",
    "## Read CSV file without whitespace handling options\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Manually trim whitespaces from specific columns\n",
    "df_trimmed = df.withColumn(\"name\", trim(df[\"name\"])) \\\n",
    "               .withColumn(\"email\", trim(df[\"email\"]))\n",
    "\n",
    "               #In this example, the name and email columns are manually trimmed of leading and trailing whitespaces.\n",
    "\n",
    "#  Handling Whitespaces in Column Names\n",
    "\n",
    "# Read CSV file with header\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Remove leading/trailing whitespaces from column names\n",
    "df = df.toDF(*[col.strip() for col in df.columns])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9822371b-9173-434c-abc3-301e099c737b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using the options Method for Multiple Configurations\n",
    "You can chain multiple configurations using the options method for a cleaner approach.\n",
    "\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "# Read CSV file with multiple configurations using options\n",
    "df = spark.read.options(\n",
    "    header=True,\n",
    "    ignoreLeadingWhiteSpace=True,\n",
    "    ignoreTrailingWhiteSpace=True,\n",
    "    inferSchema=True\n",
    ").csv(\"path/to/csvfile.csv\")\n",
    "\n",
    "df.show()\n",
    "This example uses the options method to set multiple configurations at once."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Handling whitespace issues",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
